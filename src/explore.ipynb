{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistema de detección de enlaces spam\n",
    "- El objetivo será implementar un sistema que sea capaz de detectar automáticamente si una página web contiene spam o no basándonos en su URL."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Carga de datos\n",
    "- Cargo la base de datos .csv utilizando pandas"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T09:01:24.879663Z",
     "start_time": "2025-11-21T09:01:23.509587Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "total_data = pd.read_csv('https://breathecode.herokuapp.com/asset/internal-link?id=932&path=url_spam.csv')\n",
    "total_data.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 url  is_spam\n",
       "0  https://briefingday.us8.list-manage.com/unsubs...     True\n",
       "1                             https://www.hvper.com/     True\n",
       "2                 https://briefingday.com/m/v4n3i4f3     True\n",
       "3   https://briefingday.com/n/20200618/m#commentform    False\n",
       "4                        https://briefingday.com/fan     True"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>is_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://briefingday.us8.list-manage.com/unsubs...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.hvper.com/</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://briefingday.com/m/v4n3i4f3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://briefingday.com/n/20200618/m#commentform</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://briefingday.com/fan</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Procesamiento de datos\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T09:01:24.923484Z",
     "start_time": "2025-11-21T09:01:24.917694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convertimos la columna categórica/booleana a numérica (True=1, False=0)\n",
    "total_data[\"is_spam\"] = total_data[\"is_spam\"].apply(lambda x: 1 if x == True else 0)\n",
    "\n",
    "# Verificamos el balance de clases\n",
    "print(total_data[\"is_spam\"].value_counts())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_spam\n",
      "0    2303\n",
      "1     696\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dado que una pagina web tiene otro tipo de caracteres (a diferencia del correo como se vio en la lectura de la academia), utilizaré otro tipo de preprocesado para separar palabras claras como \"google, login, free etc.\""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T09:09:41.771306Z",
     "start_time": "2025-11-21T09:06:53.262066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "nltk_data_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'nltk_data'))\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_url(url):\n",
    "    # 1. Eliminar prefijos de protocolo (https://, www.) para dejar solo el contenido\n",
    "    url = re.sub(r'https?://(www\\.)?', '', url)\n",
    "\n",
    "    # 2. Dividir la URL por caracteres especiales (puntos, guiones, barras, etc.)\n",
    "    tokens = re.split(r'[./\\-_?=&]', url)\n",
    "\n",
    "    # 3. Eliminar tokens vacíos o espacios\n",
    "    tokens = [token for token in tokens if token]\n",
    "\n",
    "    # 4. Lematización y eliminación de stopwords\n",
    "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    return \" \".join(processed_tokens)\n",
    "\n",
    "# Aplicamos la transformación\n",
    "# Asumo que 'total_data' está definido en una celda anterior\n",
    "total_data[\"url_cleaned\"] = total_data[\"url\"].apply(preprocess_url)\n",
    "\n",
    "print(total_data[[\"url\", \"url_cleaned\"]].head())"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060]\n",
      "[nltk_data]     Se produjo un error durante el intento de conexión ya\n",
      "[nltk_data]     que la parte conectada no respondió adecuadamente tras\n",
      "[nltk_data]     un periodo de tiempo, o bien se produjo un error en la\n",
      "[nltk_data]     conexión establecida ya que el host conectado no ha\n",
      "[nltk_data]     podido responder>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10060] Se\n",
      "[nltk_data]     produjo un error durante el intento de conexión ya que\n",
      "[nltk_data]     la parte conectada no respondió adecuadamente tras un\n",
      "[nltk_data]     periodo de tiempo, o bien se produjo un error en la\n",
      "[nltk_data]     conexión establecida ya que el host conectado no ha\n",
      "[nltk_data]     podido responder>\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001B[93mstopwords\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/stopwords\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Usuario/nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\Documents\\\\Git Hub Repositories\\\\machine-learning-nlp\\\\.venv\\\\nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\Documents\\\\Git Hub Repositories\\\\machine-learning-nlp\\\\.venv\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\Documents\\\\Git Hub Repositories\\\\machine-learning-nlp\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\Documents\\\\Git Hub Repositories\\\\machine-learning-nlp\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mLookupError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Git Hub Repositories\\machine-learning-nlp\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001B[39m, in \u001B[36mLazyCorpusLoader.__load\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     83\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m84\u001B[39m     root = \u001B[43mnltk\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfind\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msubdir\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mzip_name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     85\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Git Hub Repositories\\machine-learning-nlp\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001B[39m, in \u001B[36mfind\u001B[39m\u001B[34m(resource_name, paths)\u001B[39m\n\u001B[32m    578\u001B[39m resource_not_found = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m579\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(resource_not_found)\n",
      "\u001B[31mLookupError\u001B[39m: \n**********************************************************************\n  Resource \u001B[93mstopwords\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/stopwords.zip/stopwords/\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Usuario/nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\Documents\\\\Git Hub Repositories\\\\machine-learning-nlp\\\\.venv\\\\nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\Documents\\\\Git Hub Repositories\\\\machine-learning-nlp\\\\.venv\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\Documents\\\\Git Hub Repositories\\\\machine-learning-nlp\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\Documents\\\\Git Hub Repositories\\\\machine-learning-nlp\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mLookupError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[27]\u001B[39m\u001B[32m, line 14\u001B[39m\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m nltk_data_path \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m nltk.data.path:\n\u001B[32m     11\u001B[39m     nltk.data.path.append(nltk_data_path)\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m stop_words = \u001B[43mstopwords\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwords\u001B[49m(\u001B[33m\"\u001B[39m\u001B[33menglish\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     15\u001B[39m lemmatizer = WordNetLemmatizer()\n\u001B[32m     17\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpreprocess_url\u001B[39m(url):\n\u001B[32m     18\u001B[39m     \u001B[38;5;66;03m# 1. Eliminar prefijos de protocolo (https://, www.) para dejar solo el contenido\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Git Hub Repositories\\machine-learning-nlp\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py:120\u001B[39m, in \u001B[36mLazyCorpusLoader.__getattr__\u001B[39m\u001B[34m(self, attr)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m attr == \u001B[33m\"\u001B[39m\u001B[33m__bases__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    118\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mLazyCorpusLoader object has no attribute \u001B[39m\u001B[33m'\u001B[39m\u001B[33m__bases__\u001B[39m\u001B[33m'\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__load\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    121\u001B[39m \u001B[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001B[39;00m\n\u001B[32m    122\u001B[39m \u001B[38;5;66;03m# __class__ to something new:\u001B[39;00m\n\u001B[32m    123\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, attr)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Git Hub Repositories\\machine-learning-nlp\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001B[39m, in \u001B[36mLazyCorpusLoader.__load\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     84\u001B[39m             root = nltk.data.find(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.subdir\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mzip_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     85\u001B[39m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m86\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m     88\u001B[39m \u001B[38;5;66;03m# Load the corpus.\u001B[39;00m\n\u001B[32m     89\u001B[39m corpus = \u001B[38;5;28mself\u001B[39m.__reader_cls(root, *\u001B[38;5;28mself\u001B[39m.__args, **\u001B[38;5;28mself\u001B[39m.__kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Git Hub Repositories\\machine-learning-nlp\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001B[39m, in \u001B[36mLazyCorpusLoader.__load\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     79\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     80\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m81\u001B[39m         root = \u001B[43mnltk\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfind\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msubdir\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     82\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m     83\u001B[39m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Git Hub Repositories\\machine-learning-nlp\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001B[39m, in \u001B[36mfind\u001B[39m\u001B[34m(resource_name, paths)\u001B[39m\n\u001B[32m    577\u001B[39m sep = \u001B[33m\"\u001B[39m\u001B[33m*\u001B[39m\u001B[33m\"\u001B[39m * \u001B[32m70\u001B[39m\n\u001B[32m    578\u001B[39m resource_not_found = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m579\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(resource_not_found)\n",
      "\u001B[31mLookupError\u001B[39m: \n**********************************************************************\n  Resource \u001B[93mstopwords\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/stopwords\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Usuario/nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\Documents\\\\Git Hub Repositories\\\\machine-learning-nlp\\\\.venv\\\\nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\Documents\\\\Git Hub Repositories\\\\machine-learning-nlp\\\\.venv\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\Documents\\\\Git Hub Repositories\\\\machine-learning-nlp\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\Documents\\\\Git Hub Repositories\\\\machine-learning-nlp\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Vectorización (TF-IDF)\n",
    "vectorizer = TfidfVectorizer(max_features=5000, max_df=0.8, min_df=5)\n",
    "X = vectorizer.fit_transform(total_data[\"url_cleaned\"]).toarray()\n",
    "y = total_data[\"is_spam\"]\n",
    "\n",
    "# División en Train y Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Construyendo modelo SVM\n",
    "- De momento entrenare un modelo con los parametros por defecto solamente utilizando la semilla randomstate 42"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Inicialización con parámetros por defecto\n",
    "model = SVC(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicción\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluación inicial\n",
    "print(f\"Accuracy base: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Optimizacion del modelo (hiperparametros)\n",
    "- Aquí usare GridSearchCV."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definimos la cuadrícula de hiperparámetros\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto'] # Relevante para kernel rbf\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=SVC(random_state=42), param_grid=param_grid, cv=3, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Mejores parámetros encontrados: {grid_search.best_params_}\")\n",
    "print(f\"Mejor puntuación (accuracy): {grid_search.best_score_}\")\n",
    "\n",
    "# Guardamos el mejor modelo en una variable\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Guardado del modelo\n",
    "- Utilizando la libreria pickle, guardare el modelo como svm_url_spam.pkl"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pickle import dump\n",
    "\n",
    "# Guardamos el modelo optimizado\n",
    "dump(best_model, open(\"../models/svm_url_spam_optimized.sav\", \"wb\"))\n",
    "print(\"Modelo guardado exitosamente.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('3.8.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
